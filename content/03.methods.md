## Methods {.page_break_before}

### Data collection

As a reference standard, we used annotations from Search Tag Analyze Resource for GEO (STARGEO)[@doi:10.1038/sdata.2017.125]. In STARGEO, biomedical graduate students manually curate sample metadata from GEO datasets. We used these annotations to identify datasets that had been associated with a given phenotype. To represent different types of queries that researchers might perform in GEO, we identified phenotypes that resulted in a small, medium, or large number of GEO series. We limited queries to human phenotypes. We also sought to represent diverse phenotypic categories. On XX[TODO: Please indicate exact or approximate date], we identified two phenotypes with ~100 series, two with ~20 series, and two with fewer than 10 series[Table @tbl:example-id]. Because STARGEO is an ongoing project, it is likely that additional articles will be associated with these tags over time. For each GEO series, we used the STAR application programming interface[@url:http://STARGEO.org/api_docs/] to download the associated abstract, title, and GEO accession number. 

| *STARGEO tag(s)*               | *Number of GEO series*  |
|:-------------------------------|------------------------:|
| Family History + Breast Cancer |                       6 |
| Liver Damage + Hepatitis       |                       9 |
| Monozygotic Twins              |                      25 |
| Kidney + Tumor + Cell Line     |                      16 |
| Diabetes + Type 1              |                      97 |
| Osteosarcoma                   |                     112 |

Table: Caption for this example table. {#tbl:example-id}

### Model Collection

Once the data was collected the next step was identifying the techniques to be tested. Research has already shown that a variety of natural language processing models are effective on biomedical literature [@doi:10.18653/v1/W16-2922; @doi:10.1016/j.jbi.2018.09.008].
However we felt it was necessary to test a variety of techniques for both keyword extraction and word vector generation to identify the combination most
suited to the task of collecting related genomic data.

Keyword Identification is the first step that our data in the dictionary goes through. To accomplish consistent querying in a standardized framework we used the PKE package available
in Python. All documentation is available on GitHub [@url:https://github.com/boudinfl/pke]. The PKE packages allowed us to use one single package to access all the keyword embedding 
techniques, instead of individually querying each technique. The keyword extraction techniques that were tested are the following: TFIDF, KP-Miner, YAKE, TextRank, SingleRank, 
TopicRank, TopicalPageRank, PositionRank and MultipartiteRank [@url:https://github.com/boudinfl/pke]. Each of these techniques uses a different algorithm to identify keywords, 
an example of the diversity of returned keywords is available in the appendix. 

After the keywords have been identified from the target text, word vectors are generated from each keyword using a word vector model. These models use large amounts
of unlabeled text to identify the meanings of words and express those as a numeric vector. For our project we selected 6 different models that encompass a variety of techniques 
and training data. The two major frameworks used in this project were SpaCy and FastText. These models are the two most widely used frameworks in natural language processing
and both have been used in biomedical applications [@doi:10.1038/s41598-019-47046-2; @doi:10.1186/s12859-018-2496-4; @doi:10.18653/v1/W16-2922]. However there are differences 
in how these models train on sample text and generate word vectors. For these reason we chose to test both platforms. 

The source of training data is an important aspect of generating word vectors. Recent literature supports matching the training data to the testing data [@doi:10.18653/v1/W16-2922]. 
However the benefit of keeping the training and testing data in the same domain is not supported in all the literature [@doi:10.1016/j.jbi.2018.09.008]. To test this effect we have 
models trained on both biomedical literature and other sources such as web blogs, news, and Wikipedia [@url:https://arxiv.org/abs/1607.04606]. This training data has several 
possible algorithms for processing and generating word vectors. The two majors options are Continuous-Bag-Of-Words (CBOW) and SKIPGRAM, a complete explanation of these algorithms
is outside the scope of this paper. Both of these algorithms have been shown effective on biomedical natural language processing, but small differences have been shown between the
word vectors generated from either algorithm [@doi:10.1371/journal.pone.0220976]. Due to this, both algorithms are tested. There are a total of 6 models tested in this paper. FastText
and Spacy are compared head to head, as well as different algorithms and training data. A summary of each model and brief details are shown below. 

| *Model* | *Summary* |
|:--------|:---------:|
| BioWordVec        | FastText Model trained on generic biomedical data with SKIPGRAM   |
| FastTextWiki      | FastText model trained on Wikipedia data with CBOW                |
| FastTextCBOW      | FastText model trained on GEO data using CBOW                     |
| FastTextSKIPGRAM  | FastText model trained on GEO data using SKIPGRAM                 |
| SciSpacy          | A Spacy model designed for usage in biomedical applications       |
| SpacyWebLG        | A Spacy model desined for generic usage                           |

### Vector Generation

The initial step of vector generation is the identification of the top ten keywords from the text source. Each one of these keywords is turned into a word vector. This word vector is
stored as a list of numeric values. The models vary in the length of this numeric vectors from 100 - 300. Once each vector has been generated from each keyword these vectors are added 
element-wise and then divided by the number of keywords. This technique has been shown to be the simplest and most accurate way to generate a singular word vector from various word 
embeddings and is usually used to generate document-wide word embeddings [@url:http://arxiv.org/abs/1607.05368]. 

### Model Evaluation 

All model evaluation is performed in a Docker container to allow other researchers to perform the same analysis described in this section [@doi:10.1145/2723872.2723882].The Docker image 
used to build the container is the python:3.8.5 image available on the Docker website [@url:https://hub.docker.com/_/python]. Running the docker container as pulled from github will run a bash script that performs the following
steps. 
    1. STARGEO is queried to prepare the six queries. The prepareQueryData.py script takes two arguments. The first is a list of GEO identifiers and the second is the query number that these identifiers
    should belong to. PrepareQueryData.py creates a file system that contains all the abstract and titles of the series that correspond to each identifier. The file system will put each text file into the directory for the corresponding
    query. This script also randomly selects half of the data to be used as the training data. 
    2. GetGeoQueries.py is run. This script uses text files generated by GEO to evaluate the performance of GEO. A detailed explanation of this is found below in the manual comparision section.
    3. A do loop iterates over the numbers 10,20, and 30. This numbers are the number of keywords that each model should try to identify from the text. Each iteration performs the following analysis.
        i. Six scripts that correspond to SciSpaCy, BioWordVec, FastTextWiki, SpaCy, FastTextSkipGram, and FastTextCBOW are run. Each of these scripts takes the following three arguments : number of keywords, vector size, and number of STARGEO articles. Each script performs the following steps:
            a. All candidate articles from STARGEO are queried
            b. The specific word vector model is loaded (SciSpaCy, BioWordVec, ...)
            c. For each query and keyword combination findSimilarity() is run in Helper.py and added to a multiprocessing thread. This script prints to an output file the calculated similarity of each article using each combination
            d. The top 1,10 and 100 articles are returned to compare against the articles that STARGEO previously identified as related. 

### Reduced Set Testing

The results contained within this paper are from a reduced set of all STARGEO articles (266) plus an additional 1000 randomly queried articles from GEO. The purpose for performing the reduced set was the full 41,823 article corpus from STARGEO
ran for over one month and we were not able to complete the full testing. A reduced corpus of 1000 articles allowed us to compare the various methods head to head without the need for extensively long wait times. However the analysis is set up 
in such a way as to allow the researcher to easily change the amount of articles used in the analysis. 

### Manual Gene Expression Omnibus Evaluation

Gene Expression Omnibus (GEO) is the parent corpus from which STARGEO is derived [@doi:10.1093/nar/30.1.207]. To compare our technique directly to GEO we use a manual evaluation. We first use the advanced search option on 
GEO to input the exact queries we used from STARGEO. To maintain consistency with STARGEO, the results are limited to series and human genomic data. A summary file of all the results is downloaded and analyzed. To ensure equal comparision
the results are filtered to only include those datasets that exist in STARGEO's corpus while excluding SuperSeries. Using the same technique for the STARGEO evaluation the top 1,10, and 100 articles are identified and compared against the relevant articles 
from STARGEO.  

