## Methods {.page_break_before}
### Data Collection
To measure the accuracy of our method, we only used data that had already been manually identified as related. To achieve this
we used the Search Tag Analyze Resource for GEO application (StarGEO). StarGEO is a collection of datasets from the Gene Expression Omnibus that have manually annotated 
by biomedical graduate students to facilitate the task of collecting related datasets [@doi:10.1038/sdata.2017.125]. For each entry in StarGEO the dataset has an abstract, 
title, and Gene Expression Omnibus accession number.
Each dataset in StarGEO has been hand curated and attached to several tags that seek to categorize on a broad level the type of data available [@doi:10.1038/sdata.2017.125].
To ensure a mixture of broad and narrow queries, as well as different domains, we selected six queries to test. While StarGEO has a variety of species to choose from, all queries were filtered to only return human genomic data. Two of which are broad with ~100 articles returned, two are medium with ~20 articles returned and two are small with
 ~10 articles returned. These queries cover a wide range of domains. It is important to to note that since StarGEO is an
ongoing project, it is likely that these queries currently have more articles than at the time of writing. 

| *Keywords* | *Number of Articles Returned by StarGeo* |
|:-----------|:----------------------------------------:|
| Family History + Breast Cancer | 6  |
| Liver Damage + Hepatitis       | 9  |
| Monozygotic Twins              | 25 |
| Kidney + Tumor + Cell Line     | 16 |
| Diabetes + Type 1              | 97 |
| Osteosarcoma                   | 112|
The purpose of widely varying the amount of articles returned is to ensure that our method works for narrowly defined topics as well as broad topics. All data
accession was performed using the StarGEO API [@url:http://stargeo.org/api_docs/]. The API was accessed and stored in a dictionary keyed by accession number to combined abstract and title.

### Model Collection
Once the data was collected the next step was identifying the techniques to be tested. Research has already shown that a variety of natural language processing models are effective on biomedical literature [@doi:10.18653/v1/W16-2922; @doi:10.1016/j.jbi.2018.09.008].
However we felt it was necessary to test a variety of techniques for both keyword extraction and word vector generation to identify the combination most
suited to the task of collecting related genomic data.

Keyword Identification is the first step that our data in the dictionary goes through. To accomplish consistent querying in a standardized framework we used the PKE package available
in Python. All documentation is available on GitHub [@url:https://github.com/boudinfl/pke]. The PKE packages allowed us to use one single package to access all the keyword embedding 
techniques, instead of individually querying each technique. The keyword extraction techniques that were tested are the following: TFIDF, KP-Miner, YAKE, TextRank, SingleRank, 
TopicRank, TopicalPageRank, PositionRank and MultipartiteRank [@url:https://github.com/boudinfl/pke]. Each of these techniques uses a different algorithm to identify keywords, 
an example of the diversity of returned keywords is available in the appendix. 

After the keywords have been identified from the target text, word vectors are generated from each keyword using a word vector model. These models use large amounts
of unlabeled text to identify the meanings of words and express those as a numeric vector. For our project we selected 6 different models that encompass a variety of techniques 
and training data. The two major frameworks used in this project were SpaCy and FastText. These models are the two most widely used frameworks in natural language processing
and both have been used in biomedical applications [@doi:10.1038/s41598-019-47046-2; @doi:10.1186/s12859-018-2496-4; @doi:10.18653/v1/W16-2922]. However there are differences 
in how these models train on sample text and generate word vectors. For these reason we chose to test both platforms. 

The source of training data is an important aspect of generating word vectors. Recent literature supports matching the training data to the testing data [@doi:10.18653/v1/W16-2922]. 
However the benefit of keeping the training and testing data in the same domain is not supported in all the literature [@doi:10.1016/j.jbi.2018.09.008]. To test this effect we have 
models trained on both biomedical literature and other sources such as web blogs, news, and Wikipedia [@url:https://arxiv.org/abs/1607.04606]. This training data has several 
possible algorithms for processing and generating word vectors. The two majors options are Continuous-Bag-Of-Words (CBOW) and SKIPGRAM, a complete explanation of these algorithms
is outside the scope of this paper. Both of these algorithms have been shown effective on biomedical natural language processing, but small differences have been shown between the
word vectors generated from either algorithm [@doi:10.1371/journal.pone.0220976]. Due to this, both algorithms are tested. There are a total of 6 models tested in this paper. FastText
and Spacy are compared head to head, as well as different algorithms and training data. A summary of each model and brief details are shown below. 

| *Model* | *Summary* |
|:--------|:---------:|
| BioWordVec        | FastText Model trained on generic biomedical data with SKIPGRAM   |
| FastTextWiki      | FastText model trained on Wikipedia data with CBOW                |
| FastTextCBOW      | FastText model trained on GEO data using CBOW                     |
| FastTextSKIPGRAM  | FastText model trained on GEO data using SKIPGRAM                 |
| SciSpacy          | A Spacy model designed for usage in biomedical applications       |
| SpacyWebLG        | A Spacy model desined for generic usage                           |

### Vector Generation
The initial step of vector generation is the identification of the top ten keywords from the text source. Each one of these keywords is turned into a word vector. This word vector is
stored as a list of numeric values. The models vary in the length of this numeric vectors from 100 - 300. Once each vector has been generated from each keyword these vectors are added 
element-wise and then divided by the number of keywords. This technique has been shown to be the simplest and most accurate way to generate a singular word vector from various word 
embeddings and is usually used to generate document-wide word embeddings [@url:http://arxiv.org/abs/1607.05368]. 

### Model Evaluation 
All model evaluation is performed in a Docker container to allow other researchers to perform the same analysis described in this section [@doi:10.1145/2723872.2723882].The Docker image 
used to build the container is the python:3.8.5 image available on the Docker website [@url:https://hub.docker.com/_/python]. Running the docker container as pulled from github will run a bash script that performs the following
steps. 
    1. StarGEO is queried to prepare the six queries. The prepareQueryData.py script takes two arguments. The first is a list of GEO identifiers and the second is the query number that these identifiers
    should belong to. PrepareQueryData.py creates a file system that contains all the abstract and titles of the series that correspond to each identifier. The file system will put each text file into the directory for the corresponding
    query. This script also randomly selects half of the data to be used as the training data. 
    2. GetGeoQueries.py is run. This script uses text files generated by GEO to evaluate the performance of GEO. A detailed explanation of this is found below in the manual comparision section.
    3. A do loop iterates over the numbers 10,20, and 30. This numbers are the number of keywords that each model should try to identify from the text. Each iteration performs the following analysis.
        i. Six scripts that correspond to SciSpaCy, BioWordVec, FastTextWiki, SpaCy, FastTextSkipGram, and FastTextCBOW are run. Each of these scripts takes the following three arguments : number of keywords, vector size, and number of StarGEO articles. Each script performs the following steps:
            a. All candidate articles from StarGEO are queried
            b. The specific word vector model is loaded (SciSpaCy, BioWordVec, ...)
            c. For each query and keyword combination findSimilarity() is run in Helper.py and added to a multiprocessing thread. This script prints to an output file the calculated similarity of each article using each combination
            d. The top 1,10,100, and 500 articles are returned to compare against the articles that StarGEO previously identified as related. 

### Reduced Set Testing
The results contained within this paper are from a reduced set of all StarGEO articles (266) plus an additional 1000 randomly queried articles from GEO. The purpose for performing the reduced set was the full 46 article corpus from StarGEO
ran for over one month and was not able to complete the full testing. A reduced corpus of 1000 articles allowed us to compare the various methods head to head without the need for extensively long wait times. However the analysis is set up 
in such a way as to allow the researcher to easily change the amount of articles used in the analysis. 
#FIXME -> I don't know if I like how I wrote this bit. Does this belong in the discussion?

### Manual Gene Expression Omnibus Evaluation
Gene Expression Omnibus (GEO) is the parent corpus from which StarGEO is derived [@doi:10.1093/nar/30.1.207]. To compare our technique directly to GEO we use a manual evaluation. We first use the advanced search option on 
GEO to input the exact queries we used from StarGEO. To maintain consistency with StarGEO, the results are limited to series and human genomic data. A summary file of all the results is downloaded and analyzed. To ensure equal comparision
the results are filtered to only include those datasets that exist in StarGEO's corpus while excluding SuperSeries. Using the same technique for the StarGEO evaluation the top 1,10,100,500 articles are identified and compared against the relevant articles 
from StarGEO.  

